{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### betöltés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'False'\n"
     ]
    }
   ],
   "source": [
    "%%script False\n",
    "\n",
    "%pip install nltk\n",
    "\n",
    "%pip install spacy==3.7.5\n",
    "%pip install hu_core_news_lg@https://huggingface.co/huspacy/hu_core_news_lg/resolve/v3.7.0/hu_core_news_lg-any-py3-none-any.whl?download=true\n",
    "%pip install xx_ent_wiki_sm@https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.7.0/xx_ent_wiki_sm-3.7.0-py3-none-any.whl?download=true\n",
    "\n",
    "%pip install transformers sentencepiece flax\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "%pip install pandas\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('JanosVitez.txt', 'r', encoding='utf-8') as file:\n",
    "    # A teljes fájl tartalmának beolvasása\n",
    "    data = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fejezetek mentén elvágom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tüzesen süt le a nyári nap sugára\n",
      "Az ég tetejéről a juhászbojtárra.\n",
      "Fölösleges dolog sütnie oly na\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# A reguláris kifejezés, ami a számokat keresi\n",
    "pattern = r'\\b([1-9]|1[0-9]|2[0-7])\\b'\n",
    "\n",
    "# A szöveg szétbontása a reguláris kifejezés mentén\n",
    "data_parts = [part for part in re.split(pattern, data) if len(part) >= 100]\n",
    "\n",
    "print(data_parts[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tokenizálás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BP5589\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\BP5589\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\nTüzesen süt le a nyári nap sugára\\nAz ég tetejéről a juhászbojtárra.', 'Fölösleges dolog sütnie oly nagyon,\\nA juhásznak úgyis nagy melege vagyon.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = sent_tokenize(data_parts[0])   # mondatokra bontás\n",
    "print(tokenized_sentence[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tüzesen', 'süt', 'le', 'a', 'nyári', 'nap', 'sugára', 'Az', 'ég', 'tetejéről', 'a', 'juhászbojtárra', '.', 'Fölösleges', 'dolog']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = word_tokenize(data_parts[0])   # mondatokra bontás\n",
    "print(tokenized_word[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "összes szó száma: 343\n",
      "egyedi szavak száma: 221\n"
     ]
    }
   ],
   "source": [
    "print(f'összes szó száma: {len(tokenized_word)}')\n",
    "print(f'egyedi szavak száma: {len(set(tokenized_word))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stopszavak eldobása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BP5589\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_words = [word for word in tokenized_word if word.lower() not in stopwords.words('hungarian')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "összes szó száma (-stop): 265\n",
      "egyedi szavak száma (-stop): 184\n"
     ]
    }
   ],
   "source": [
    "print(f'összes szó száma (-stop): {len(meaningful_words)}')\n",
    "print(f'egyedi szavak száma (-stop): {len(set(meaningful_words))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tüzesen', 'süt', 'le', 'nyári', 'nap', 'sugára', 'ég', 'tetejéről', 'juhászbojtárra', '.', 'Fölösleges', 'dolog', 'sütnie', 'oly', ',']\n"
     ]
    }
   ],
   "source": [
    "print(meaningful_words[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import HungarianStemmer\n",
    "stemmer = nltk.stem.SnowballStemmer('hungarian')\n",
    "stemmed_word = [stemmer.stem(word) for word in meaningful_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "összes szó száma (stemmed): 265\n",
      "egyedi szavak száma (stemmed): 167\n"
     ]
    }
   ],
   "source": [
    "print(f'összes szó száma (stemmed): {len(stemmed_word)}')\n",
    "print(f'egyedi szavak száma (stemmed): {len(set(stemmed_word))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tüzes', 'sü', 'le', 'nyár', 'nap', 'sug', 'ég', 'tetejéről', 'juhászbojtár', '.', 'fölösleges', 'dolog', 'sütn', 'oly', ',']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_word[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### HuSpacy token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_hu = spacy.load(\"hu_core_news_lg\")\n",
    "#nlp_hu = spacy.load(\"xx_ent_wiki_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Text  Lemme   POS  Is_Stop     norm\n",
      "0  Tüzesen  Tüzes   ADJ    False  tüzesen\n",
      "1      süt    süt  VERB    False      süt\n",
      "2    nyári  nyári   ADJ    False    nyári\n",
      "3      nap    nap  NOUN    False      nap\n",
      "4   sugára  sugár  NOUN    False   sugára\n",
      "['tüzes', 'süt', 'nyári', 'nap', 'sugár', 'ég', 'tető', 'juhászbojtár', 'fölösleges', 'dolog', 'süt', 'juhász', 'úgyis', 'meleg', 'vagyon']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "doc = nlp_hu(data_parts[0])\n",
    "\n",
    "meaningful_pos = {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "df_tokens = pd.DataFrame([[token.text, token.lemma_, token.pos_, token.is_stop, token.norm_] for token in doc if token.pos_ in meaningful_pos and token.is_alpha and not token.is_stop],   \n",
    "                         columns=['Text', 'Lemme', 'POS', 'Is_Stop', 'norm'])\n",
    "print(df_tokens.head(5))\n",
    "\n",
    "print(df_tokens['Lemme'].str.lower().to_list()[:15]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### szómátrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0    1    2    3    4    5    6    7    8    9  ...   18    19   20  \\\n",
      "lemma                                                     ...                   \n",
      "vitéz   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  9.0   4.0  7.0   \n",
      "lát     NaN  NaN  1.0  6.0  NaN  5.0  1.0  1.0  NaN  NaN  ...  7.0   6.0  3.0   \n",
      "nap     1.0  1.0  1.0  NaN  8.0  1.0  4.0  NaN  NaN  1.0  ...  2.0   1.0  2.0   \n",
      "király  NaN  NaN  NaN  NaN  NaN  NaN  NaN  2.0  NaN  NaN  ...  NaN  11.0  NaN   \n",
      "megy    1.0  1.0  NaN  1.0  1.0  NaN  5.0  2.0  1.0  1.0  ...  2.0   1.0  1.0   \n",
      "\n",
      "         21   22   23   24   25   26  row_sum  \n",
      "lemma                                          \n",
      "vitéz   1.0  2.0  1.0  1.0  NaN  5.0     49.0  \n",
      "lát     NaN  1.0  NaN  2.0  1.0  4.0     47.0  \n",
      "nap     1.0  1.0  NaN  NaN  2.0  1.0     40.0  \n",
      "király  NaN  NaN  NaN  NaN  NaN  1.0     36.0  \n",
      "megy    NaN  NaN  NaN  1.0  NaN  2.0     32.0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "meaningful_pos = {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "df_words = pd.DataFrame(columns=['lemma']).set_index('lemma')\n",
    "\n",
    "for idx, part in enumerate(data_parts):\n",
    "    doc = nlp_hu(part)\n",
    "    lemmas = [token.lemma_.lower() for token in doc if token.pos_ in meaningful_pos and token.is_alpha and not token.is_stop]\n",
    "    lemmas_set = list(set(lemmas))\n",
    "    \n",
    "    df_lemmas_set = pd.DataFrame({'lemma': lemmas_set})\n",
    "    df_lemmas_set[idx] = df_lemmas_set['lemma'].apply(lambda x: lemmas.count(x))\n",
    "    \n",
    "    df_words = df_words.join(df_lemmas_set.set_index('lemma'), how='outer')\n",
    "\n",
    "\n",
    "# Calculate the row sums and order the dataframe by these sums\n",
    "df_words['row_sum'] = df_words.sum(axis=1)\n",
    "df_words = df_words.sort_values(by='row_sum', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_words.to_excel('JanosVitez_words.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0           1            2            3         4\n",
      "0            jön       patak          tud         szép       víz\n",
      "1           kend         szó         tesz      szegény      száj\n",
      "2          gazda         tud          szó       gondol     dolog\n",
      "3            lát         jut         szép          ész     könny\n",
      "4            nap          tó        sötét           ég     közép\n",
      "5        zsivány        élet          lát          ház      szép\n",
      "6           megy         nap           ló        vezér    katona\n",
      "7         magyar       tatár        sereg          nép    király\n",
      "8          hideg        megy          néz        sereg      vesz\n",
      "9        csillag        föld         hegy          tud       jut\n",
      "10        király       török       ország      francia    magyar\n",
      "11         török          ló         basa  királylyány       fut\n",
      "12        király       vitéz          szó          nap     ekkép\n",
      "13         világ         kis          ugy          tud      szép\n",
      "14        király       vitéz  királylyány        marad      zsák\n",
      "15         vitéz        megy         szép       gondol  gondolat\n",
      "16         vitéz         tud        isten        felhő      tető\n",
      "17           nap          ér        világ       kedves       kis\n",
      "18         vitéz         lát         szem        óriás     amint\n",
      "19        király       óriás          lát        vitéz      mond\n",
      "20         vitéz  boszorkány        óriás       ország       tűz\n",
      "21         világ         szó        amint       ballag      hold\n",
      "22        tenger         tud          áll        vitéz     óriás\n",
      "23          visz       óriás         part         tesz     felel\n",
      "24          kapu         áll      sárkány          lát      élet\n",
      "25            él         nap        világ         föld     könny\n",
      "26         vitéz         lát       kedves           tó     rózsa\n",
      "row_sum    vitéz         lát          nap       király      megy\n"
     ]
    }
   ],
   "source": [
    "# Transpose the df_words DataFrame\n",
    "df_words_transposed = df_words.T\n",
    "\n",
    "# Create a new DataFrame to store the top 5 words for each row\n",
    "top_words_df = pd.DataFrame(index=df_words_transposed.index, columns=range(5))\n",
    "\n",
    "# Iterate over each row in the transposed DataFrame\n",
    "for idx, row in df_words_transposed.iterrows():\n",
    "    # Get the top 5 words with the highest values\n",
    "    top_words = row.nlargest(5).index.tolist()\n",
    "    # Assign the top words to the new DataFrame\n",
    "    top_words_df.loc[idx] = top_words\n",
    "\n",
    "print(top_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_words_df.to_excel('JanosVitez_top_words.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extractiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/itsmohammadshahid/nlp-text-summarizer-using-spacy\n",
    "from spacy.lang.hu.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "def textSummarizer(text, percentage=0):\n",
    "    \n",
    "    # load the model into spaCy\n",
    "    nlp = spacy.load('hu_core_news_lg')\n",
    "    \n",
    "    # pass the text into the nlp function\n",
    "    doc= nlp(text)\n",
    "    \n",
    "    ## The score of each word is kept in a frequency table\n",
    "    tokens=[token.text for token in doc]\n",
    "    freq_of_word=dict()\n",
    "    \n",
    "    # Text cleaning and vectorization \n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation and word.is_alpha:\n",
    "                if word.text not in freq_of_word.keys():\n",
    "                    freq_of_word[word.text] = 1\n",
    "                else:\n",
    "                    freq_of_word[word.text] += 1\n",
    "                    \n",
    "    # Maximum frequency of word\n",
    "    max_freq=max(freq_of_word.values())\n",
    "    \n",
    "    # Normalization of word frequency\n",
    "    for word in freq_of_word.keys():\n",
    "        freq_of_word[word]=freq_of_word[word]/max_freq\n",
    "        \n",
    "    # In this part, each sentence is weighed based on how often it contains the token.\n",
    "    sent_tokens= [sent for sent in doc.sents]\n",
    "    sent_scores = dict()\n",
    "    for sent in sent_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in freq_of_word.keys():\n",
    "                if sent not in sent_scores.keys():                            \n",
    "                    sent_scores[sent]=freq_of_word[word.text.lower()]\n",
    "                else:\n",
    "                    sent_scores[sent]+=freq_of_word[word.text.lower()]\n",
    "    \n",
    "    if percentage==0:\n",
    "        len_tokens=1\n",
    "    else:\n",
    "        len_tokens=int(len(sent_tokens)*percentage)\n",
    "    \n",
    "    # Summary for the sentences with maximum score. Here, each sentence in the list is of spacy.span type\n",
    "    summary = nlargest(n = len_tokens, iterable = sent_scores,key=sent_scores.get)\n",
    "    \n",
    "    # Prepare for final summary\n",
    "    final_summary=[word.text for word in summary]\n",
    "    \n",
    "    #convert to a string\n",
    "    summary=\" \".join(final_summary)\n",
    "    \n",
    "    # Return final summary\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0->1->2->3->4->5->6->7->8->9->10->11->12->13->14->15->16->17->18->19->20->21->22->23->24->25->26->"
     ]
    }
   ],
   "source": [
    "l1, l2 = [], [] \n",
    "for i, part in enumerate(data_parts):\n",
    "    print(f'{i}', end='->')\n",
    "    l1.append(i)\n",
    "    l2.append(textSummarizer(part,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "absztraktív"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GPY\\Pannon_szoveg\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text: str, model_name: str = \"google/mt5-small\") -> str:\n",
    "    # Modell és tokenizer betöltése\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenizálás és összefoglalás generálása\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=100, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:->2:->3:->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4:->5:->6:->7:->8:->9:->10:->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:->12:->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:->14:->15:->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:->17:->18:->19:->20:->21:->22:->23:->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24:->25:->26:->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    }
   ],
   "source": [
    "l3, l4 = [], []\n",
    "for i, part in enumerate(data_parts):\n",
    "    print(f'{i}:', end='->')\n",
    "    l3.append(i)\n",
    "    l4.append(summarize_text(part)[12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "összefűzöm táblázatosan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = None\n",
    "df_summary = pd.DataFrame()\n",
    "\n",
    "df_summary['part'] = l1\n",
    "df_summary['extractiv'] = l2\n",
    "df_summary['abstractive'] = l4\n",
    "\n",
    "df_summary['extractiv'] = df_summary['extractiv'].apply(lambda x: x.replace('\\n', '').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mentem excelbe az eredményt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_summary.to_excel('JanosVitez_summary.xlsx', index=False, sheet_name='Summary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
